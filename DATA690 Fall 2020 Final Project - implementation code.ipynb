{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DATA690 Fall 2020: Final Project - implementation code.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOJJy/6absPHEnHuSzhXxdy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UjPE1BWojEY","executionInfo":{"status":"ok","timestamp":1607383193665,"user_tz":300,"elapsed":4201,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}},"outputId":"eddf5b2c-dfaf-4f81-9e75-577181195a2e"},"source":["% pip install optuna\n","\n","import torch\n","import torch.nn as nn\n","import os\n","\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from statistics import mean \n","import random\n","import copy\n","\n","import optuna\n","import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","import sys\n","sys.path.append('/content/gdrive/Shared drives/DATA_690_deep_learning_final_project/')\n","\n","from snake_game_modified import SnakeGame"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: optuna in /usr/local/lib/python3.6/dist-packages (2.3.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna) (4.41.1)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.3.20)\n","Requirement already satisfied: alembic in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.3)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (1.4.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from optuna) (0.17.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (20.4)\n","Requirement already satisfied: cliff in /usr/local/lib/python3.6/dist-packages (from optuna) (3.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from optuna) (1.18.5)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.6/dist-packages (from optuna) (4.6.2)\n","Requirement already satisfied: cmaes>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from optuna) (0.7.0)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.1.3)\n","Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (1.0.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from alembic->optuna) (2.8.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->optuna) (1.15.0)\n","Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.13)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (5.5.1)\n","Requirement already satisfied: cmd2!=0.8.3,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (1.4.0)\n","Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (3.3.0)\n","Requirement already satisfied: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna) (0.7.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna) (1.1.1)\n","Requirement already satisfied: colorama>=0.3.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.4.4)\n","Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (1.8.1)\n","Requirement already satisfied: importlib-metadata>=1.6.0; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (2.0.0)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,>=0.8.0->cliff->optuna) (20.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=1.6.0; python_version < \"3.8\"->cmd2!=0.8.3,>=0.8.0->cliff->optuna) (3.4.0)\n","Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X-ObUGBaUQIv","executionInfo":{"status":"ok","timestamp":1607383193665,"user_tz":300,"elapsed":4185,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["torch.backends.cudnn.deterministic=True\n","torch.manual_seed(42)\n","np.random.seed(42)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0tMpSu2UTlv","executionInfo":{"status":"ok","timestamp":1607383193667,"user_tz":300,"elapsed":4178,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"43OvGsZZUKRp","executionInfo":{"status":"ok","timestamp":1607383193667,"user_tz":300,"elapsed":4174,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["def moveTo(obj, device):\n","    \"\"\"\n","    obj: the python object to move to a device, or to move its contents to a device\n","    device: the compute device to move objects to\n","    \"\"\"\n","    if hasattr(obj, \"to\"):\n","        return obj.to(device)\n","    elif isinstance(obj, list):\n","        return [moveTo(x, device) for x in obj]\n","    elif isinstance(obj, tuple):\n","        return tuple(moveTo(list(obj), device))\n","    elif isinstance(obj, set):\n","        return set(moveTo(list(obj), device))\n","    elif isinstance(obj, dict):\n","        to_ret = dict()\n","        for key, value in obj.items():\n","            to_ret[moveTo(key, device)] = moveTo(value, device)\n","        return to_ret\n","    else:\n","        return obj"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls1pYdtSZ4oy","executionInfo":{"status":"ok","timestamp":1607383193668,"user_tz":300,"elapsed":4167,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["class Flatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), -1)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rn17iZze8Mt6"},"source":["## Loss function\n","\n","$$[Q(s,a,\\theta)-(r(s,a)+\\gamma\\max_aQ(s',a,\\theta))]^2$$\n","\n","Reference: https://towardsdatascience.com/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677"]},{"cell_type":"code","metadata":{"id":"IZe4WRok8LSa","executionInfo":{"status":"ok","timestamp":1607383193670,"user_tz":300,"elapsed":4164,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["def bellman_loss_function(max_current_q, current_reward, max_future_q=None, gamma=.85):\n","  current_reward = moveTo(torch.tensor(current_reward), device)\n","  gamma = moveTo(torch.tensor(gamma), device)\n","  if max_future_q is not None:\n","    return torch.pow(torch.subtract(max_current_q, torch.add(current_reward, torch.multiply(gamma, max_future_q))), 2)\n","  else:\n","    return torch.pow(torch.subtract(max_current_q, current_reward), 2)"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"of34iSxMUj88"},"source":["## Batch training function"]},{"cell_type":"code","metadata":{"id":"RUe1dNTX3GWD","executionInfo":{"status":"ok","timestamp":1607383193671,"user_tz":300,"elapsed":4161,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["def train_model(trainable_model, constant_model, optimizer, device, current_states, current_rewards, future_states, batch_size, epochs=10, gamma=.85):\n","\n","  trainable_model = trainable_model.train()\n","  constant_model = constant_model.eval()\n","  current_states_tensor = moveTo(torch.tensor(current_states, dtype=torch.float), device)\n","  current_rewards_tensor = moveTo(torch.tensor(current_rewards, dtype=torch.float), device)\n","  future_states_tensor = moveTo(torch.tensor(future_states, dtype=torch.float), device)\n","\n","  loss_list = []\n","\n","  for epoch in range(epochs):\n","\n","    current_q = trainable_model(current_states_tensor)\n","\n","    with torch.no_grad():\n","      future_q = constant_model(future_states_tensor)\n","\n","    max_current_q =  moveTo(torch.zeros(batch_size, 1), device)\n","    max_future_q = moveTo(torch.zeros(batch_size, 1), device)\n","\n","    for i, q in enumerate(current_q):\n","      max_q, action = torch.max(q, 0)\n","      index = torch.tensor([i])\n","      max_current_q = max_current_q.index_put([index],max_q)\n","    for i, q in enumerate(future_q):\n","      max_q, action = torch.max(q, 0)\n","      index = torch.tensor([i])\n","      max_future_q = max_future_q.index_put([index],max_q)\n","\n","    loss = bellman_loss_function(max_current_q=max_current_q, current_reward=current_rewards, max_future_q=max_future_q, gamma=gamma)\n","    loss.mean().backward()\n","\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    loss_list.append(loss.mean().item())\n","  return trainable_model, constant_model, mean(loss_list)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_RIOZFvSUwA_"},"source":["## Generate fully connected model"]},{"cell_type":"code","metadata":{"id":"_J3rm1BAr6-I","executionInfo":{"status":"ok","timestamp":1607383193672,"user_tz":300,"elapsed":4153,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["def get_fully_connected_model(input_size=0, num_hidden_layers=20, num_nodes=250, output_size=0):\n","\n","  model = nn.Sequential(\n","    nn.Flatten(),\n","    nn.Linear(input_size,  num_nodes),\n","    nn.LeakyReLU()\n","  )\n","\n","  for hidden_layer in range(num_hidden_layers):\n","    model.add_module(f'linear_{hidden_layer}', nn.Linear(num_nodes,  num_nodes))\n","    model.add_module(f'non_linearity_{hidden_layer}', nn.LeakyReLU())\n","\n","  model.add_module(f'output', nn.Linear(num_nodes, output_size))\n","\n","  return model\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"63dE5QtewrE0"},"source":["## Reward function\n","\n","Calculate Eucledian distance between the head of the snake, then multiply its reciprocal to the rate of return, and finally add the result to the actual reward from the move. This means that as the snake gets closer, the return gets bigger but only by a tiny fraction which is controlled by the rate of return:\n","\n","$$r(s, f, v) = \\rho\\left[\\frac{1}{\\sqrt{(f_0 - s_0)^2 + (f_1 - s_1)^2}}\\right]+v$$\n","\n","Where $s$ is the position of the head of the snake, $f$ is the position of the food, $v$ is the reward returned by the game (-1, 0, or 1) and $\\rho$ is the rate that controls how much of the distance function to return."]},{"cell_type":"code","metadata":{"id":"cOm53GxXm-FS","executionInfo":{"status":"ok","timestamp":1607383193672,"user_tz":300,"elapsed":4148,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["def get_reward(snake, food, reward, rho = 0.1):\n","  snake_head = snake[0]\n","  x_distance = np.subtract(food[0], snake_head[0])\n","  y_distance = np.subtract(food[1], snake_head[1])\n","  euclidean_distance = np.sqrt(np.add(np.power(x_distance, 2), np.power(y_distance, 2)))\n","  if euclidean_distance > 0 and reward > -1:\n","    return np.add(np.multiply(np.divide(1,euclidean_distance), rho), reward)\n","    # return np.add(np.divide(1,euclidean_distance), reward)\n","  else:\n","    return reward\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyQ03FzbU9DJ"},"source":["## Process training trial\n","\n","Train model with different set of hyperparamters"]},{"cell_type":"code","metadata":{"id":"gQmj61G_QnWe","executionInfo":{"status":"ok","timestamp":1607383193878,"user_tz":300,"elapsed":4351,"user":{"displayName":"Jorge Neyra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjuu5ZxulIvlUF4kQ7Fx-JEkYMunqTV1k93wXJu=s64","userId":"16538749842847312111"}}},"source":["def process_training_trial(trial):\n","  board_width = 22\n","  board_height = 22\n","\n","  batch_size = trial.suggest_int(\"batch_size\", 8, 32)\n","\n","  epsilon = trial.suggest_float(\"epsilon\", 0.1, 0.25)\n","\n","  training_rounds = trial.suggest_int(\"training_rounds\", 50, 200)\n","\n","  num_hidden_layers = trial.suggest_int(\"num_hidden_layers\", 5, 200)\n","\n","  learning_rate = trial.suggest_float(\"learning_rate\", 0.0000001, 0.01)\n","\n","  gamma = trial.suggest_float(\"gamma\", 0.9, 0.99)\n","\n","  switch_model_threshold = trial.suggest_int(\"switch_model_threshold\", 10, 20)\n","\n","  trainable_model = get_fully_connected_model(input_size=(board_width*board_height), num_hidden_layers=num_hidden_layers, output_size=4)\n","  constant_model = get_fully_connected_model(input_size=(board_width*board_height), num_hidden_layers=num_hidden_layers, output_size=4)\n","\n","  # trainable_model = get_cnn_model(flattened_size=(board_width*board_height), output_size=4)\n","  # constant_model = get_cnn_model(flattened_size=(board_width*board_height), output_size=4)\n","\n","  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\"])\n","\n","  rho = trial.suggest_float(\"rho\", 5, 10)\n","\n","  if optimizer_name == \"Adam\":\n","    optimizer = torch.optim.Adam(trainable_model.parameters(), lr=learning_rate)\n","  elif optimizer_name == \"AdamW\":\n","    optimizer = torch.optim.AdamW(trainable_model.parameters(), lr=learning_rate)\n","  else:\n","    optimizer = torch.optim.SGD(trainable_model.parameters(), lr=learning_rate)\n","\n","  game = SnakeGame(board_width = board_width-2, board_height = board_height-2, gui = False)\n","\n","  row_index = f'{batch_size}_{round(epsilon, 3)}_{training_rounds}_{num_hidden_layers}_{round(learning_rate, 7)}_{optimizer_name}_{rho}_{gamma}_{switch_model_threshold}'\n","\n","  results_path = '/content/gdrive/Shared drives/DATA_690_deep_learning_final_project/results'\n","\n","  if os.path.exists(f'{results_path}/score_resuts_v2.csv'):\n","    score_results_df = pd.read_csv(f'{results_path}/score_resuts_v2.csv', index_col='hyperparameters')\n","  else:\n","    os.makedirs(results_path, exist_ok=True)\n","    score_results_df = pd.DataFrame(columns=['hyperparameters'])\n","    score_results_df = score_results_df.set_index('hyperparameters')\n","\n","  if os.path.exists(f'{results_path}/loss_resuts_v2.csv'):\n","    loss_results_df = pd.read_csv(f'{results_path}/loss_resuts_v2.csv', index_col='hyperparameters')\n","  else:\n","    os.makedirs(results_path, exist_ok=True)\n","    loss_results_df = pd.DataFrame(columns=['hyperparameters'])\n","    loss_results_df = loss_results_df.set_index('hyperparameters')\n","\n","  score_results = {\n","      \"games\": [],\n","      \"scores\": []\n","  }\n","\n","  loss_results = {\n","      \"rounds\": [],\n","      \"loss\": [] \n","  }\n","\n","  state = np.zeros((1,1,board_width,board_height), dtype=int)\n","\n","  done, score, snake, food, reward = game.start()\n","\n","  for snake_point in snake:\n","      state[0, 0, snake_point[0], snake_point[1]] = 1\n","\n","  state[0, 0, food[0], food[1]] = 2\n","\n","  round_index = 0\n","  game_number = 1\n","\n","  while round_index < training_rounds:\n","    \n","    current_states = np.zeros((batch_size, 1, board_width, board_height), dtype=int)\n","    rewards = np.zeros((batch_size,1), dtype=int)\n","    future_states = np.zeros((batch_size, 1, board_width, board_height), dtype=int)\n","\n","    trainable_model.to(device)\n","    constant_model.to(device)\n","    constant_model = constant_model.eval()\n","    trainable_model = trainable_model.eval()\n","    \n","    batch_index = 0\n","\n","    while batch_index < batch_size:\n","\n","      state_tensor = moveTo(torch.tensor(state, dtype=torch.float), device)\n","      \n","      if random.gauss(.5, .25) > epsilon:\n","        with torch.no_grad():\n","          q = trainable_model(state_tensor)\n","        max_q, action = torch.max(q, 1)\n","        action = action.detach().cpu().item()\n","      else:\n","        action = random.randint(0, 3)\n","\n","      done, score, snake, food, reward = game.step(action)\n","    \n","      new_state = np.zeros((1, 1, board_width,board_height), dtype=int)\n","      for snake_point in snake:\n","          new_state[0, 0, snake_point[0], snake_point[1]] = 1\n","      new_state[0, 0, food[0], food[1]] = 2\n","\n","      current_states[batch_index] = state[0]\n","      rewards[batch_index] = get_reward(snake, food, reward, rho)\n","      future_states[batch_index] = new_state[0]\n","\n","      if done:\n","        score_results[\"games\"].append(game_number)\n","        score_results[\"scores\"].append(score)\n","        score_results_df.loc[f'{row_index}', f'game_{game_number}'] = score\n","        game_number += 1\n","        game = SnakeGame(board_width = board_width-2, board_height = board_height-2, gui = False)\n","        done, score, snake, food, reward = game.start()\n","        state = np.zeros((1, 1, board_width,board_height), dtype=int)\n","        for snake_point in snake:\n","            state[0, 0, snake_point[0], snake_point[1]] = 1\n","        state[0, 0, food[0], food[1]] = 2\n","      else:\n","        state = new_state\n","      batch_index += 1\n","\n","    trainable_model, constant_model, loss = train_model(\n","        trainable_model,\n","        constant_model,\n","        optimizer, device,\n","        current_states,\n","        rewards,\n","        future_states,\n","        batch_size=batch_size,\n","        epochs=20,\n","        gamma=gamma\n","    )\n","\n","    if round_index%switch_model_threshold == 0:\n","      constant_model = copy.deepcopy(trainable_model)\n","\n","    loss_results[\"rounds\"].append(round_index)\n","    loss_results[\"loss\"].append(loss)\n","    loss_results_df.loc[f'{row_index}', f'round_{round_index}'] = loss\n","    round_index += 1\n","\n","  loss_mean = np.mean(loss_results[\"loss\"])\n","  loss_results_df.loc[f'{row_index}', f'mean'] = loss_mean\n","  loss_results_df.to_csv(f'{results_path}/loss_resuts_v2.csv')\n","\n","  scores_mean = np.mean(score_results[\"scores\"])\n","  score_results_df.loc[f'{row_index}', f'mean'] = scores_mean\n","  score_results_df.to_csv(f'{results_path}/score_resuts_v2.csv')\n","\n","  return scores_mean"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gq9d13iNVKIo"},"source":["## Hyperparameter tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G4UR4mbW2Jey","outputId":"c748cc17-0ef8-42f5-b8ad-afaa2f34e6f9"},"source":["# from https://github.com/optuna/optuna/blob/master/examples/pytorch_simple.py\n","\n","study = optuna.create_study(direction=\"maximize\")\n","study.optimize(process_training_trial, n_trials=100)\n","\n","pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n","complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n","\n","print(\"Study statistics: \")\n","print(\"  Number of finished trials: \", len(study.trials))\n","print(\"  Number of pruned trials: \", len(pruned_trials))\n","print(\"  Number of complete trials: \", len(complete_trials))\n","\n","print(\"Best trial:\")\n","trial = study.best_trial\n","\n","print(\"  Value: \", trial.value)\n","\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","    print(\"    {}: {}\".format(key, value))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[32m[I 2020-12-07 23:19:54,027]\u001b[0m A new study created in memory with name: no-name-6980852e-d192-40df-8b8e-e148606ff7b6\u001b[0m\n","\u001b[32m[I 2020-12-07 23:22:30,484]\u001b[0m Trial 0 finished with value: 0.014925373134328358 and parameters: {'batch_size': 11, 'epsilon': 0.24768646745584463, 'training_rounds': 73, 'num_hidden_layers': 191, 'learning_rate': 0.004980980134786481, 'gamma': 0.9246351942512071, 'switch_model_threshold': 18, 'optimizer': 'Adam', 'rho': 6.545472838885006}. Best is trial 0 with value: 0.014925373134328358.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:26:40,749]\u001b[0m Trial 1 finished with value: 0.04081632653061224 and parameters: {'batch_size': 11, 'epsilon': 0.1583061005868739, 'training_rounds': 159, 'num_hidden_layers': 144, 'learning_rate': 0.0009083559199512158, 'gamma': 0.9334498772750115, 'switch_model_threshold': 11, 'optimizer': 'Adam', 'rho': 7.8138787331263195}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:28:20,728]\u001b[0m Trial 2 finished with value: 0.032504780114722756 and parameters: {'batch_size': 32, 'epsilon': 0.20542495703318361, 'training_rounds': 181, 'num_hidden_layers': 24, 'learning_rate': 0.008329634835701519, 'gamma': 0.9407957072597039, 'switch_model_threshold': 10, 'optimizer': 'AdamW', 'rho': 5.974764285078579}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:30:26,133]\u001b[0m Trial 3 finished with value: 0.03424657534246575 and parameters: {'batch_size': 27, 'epsilon': 0.1426625014139825, 'training_rounds': 117, 'num_hidden_layers': 72, 'learning_rate': 0.0017264839774811622, 'gamma': 0.9829312821939635, 'switch_model_threshold': 12, 'optimizer': 'AdamW', 'rho': 7.239609849497044}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:35:11,669]\u001b[0m Trial 4 finished with value: 0.02346041055718475 and parameters: {'batch_size': 32, 'epsilon': 0.14435435282815623, 'training_rounds': 130, 'num_hidden_layers': 167, 'learning_rate': 0.008240418859944003, 'gamma': 0.9469870580286498, 'switch_model_threshold': 14, 'optimizer': 'Adam', 'rho': 6.004051853361051}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:36:50,167]\u001b[0m Trial 5 finished with value: 0.02040816326530612 and parameters: {'batch_size': 11, 'epsilon': 0.2298642384678963, 'training_rounds': 168, 'num_hidden_layers': 44, 'learning_rate': 0.009039927193143641, 'gamma': 0.9704423920812258, 'switch_model_threshold': 16, 'optimizer': 'AdamW', 'rho': 6.911921823684384}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:37:59,008]\u001b[0m Trial 6 finished with value: 0.017142857142857144 and parameters: {'batch_size': 32, 'epsilon': 0.1834668845089707, 'training_rounds': 125, 'num_hidden_layers': 26, 'learning_rate': 0.005525794785930741, 'gamma': 0.9419840003017768, 'switch_model_threshold': 12, 'optimizer': 'Adam', 'rho': 6.866279837155396}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:39:39,292]\u001b[0m Trial 7 finished with value: 0.008849557522123894 and parameters: {'batch_size': 9, 'epsilon': 0.2347525484173338, 'training_rounds': 170, 'num_hidden_layers': 78, 'learning_rate': 0.007794835117901689, 'gamma': 0.933035411414393, 'switch_model_threshold': 14, 'optimizer': 'SGD', 'rho': 6.8046419175458785}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:42:09,159]\u001b[0m Trial 8 finished with value: 0.024271844660194174 and parameters: {'batch_size': 27, 'epsilon': 0.1563414495839094, 'training_rounds': 84, 'num_hidden_layers': 132, 'learning_rate': 0.004719800758735373, 'gamma': 0.9279281363750503, 'switch_model_threshold': 12, 'optimizer': 'AdamW', 'rho': 5.054305185164692}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:42:58,249]\u001b[0m Trial 9 finished with value: 0.02356902356902357 and parameters: {'batch_size': 22, 'epsilon': 0.11231833877307879, 'training_rounds': 145, 'num_hidden_layers': 13, 'learning_rate': 0.005364180482509698, 'gamma': 0.9423591668936447, 'switch_model_threshold': 11, 'optimizer': 'AdamW', 'rho': 6.016301501938999}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:48:09,587]\u001b[0m Trial 10 finished with value: 0.011111111111111112 and parameters: {'batch_size': 16, 'epsilon': 0.10816944142646041, 'training_rounds': 200, 'num_hidden_layers': 137, 'learning_rate': 0.000310947052914153, 'gamma': 0.9056635175817729, 'switch_model_threshold': 20, 'optimizer': 'Adam', 'rho': 8.924908620044736}. Best is trial 1 with value: 0.04081632653061224.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:49:27,358]\u001b[0m Trial 11 finished with value: 0.04830917874396135 and parameters: {'batch_size': 24, 'epsilon': 0.14065269781341946, 'training_rounds': 102, 'num_hidden_layers': 78, 'learning_rate': 0.00023422784232241694, 'gamma': 0.9746045624418089, 'switch_model_threshold': 10, 'optimizer': 'SGD', 'rho': 8.399445336408505}. Best is trial 11 with value: 0.04830917874396135.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:51:01,584]\u001b[0m Trial 12 finished with value: 0.011627906976744186 and parameters: {'batch_size': 19, 'epsilon': 0.1691282129408209, 'training_rounds': 99, 'num_hidden_layers': 113, 'learning_rate': 0.0002789171383942718, 'gamma': 0.9635925125279766, 'switch_model_threshold': 10, 'optimizer': 'SGD', 'rho': 8.609538354618572}. Best is trial 11 with value: 0.04830917874396135.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:51:50,594]\u001b[0m Trial 13 finished with value: 0.032520325203252036 and parameters: {'batch_size': 25, 'epsilon': 0.12593758415065717, 'training_rounds': 60, 'num_hidden_layers': 85, 'learning_rate': 0.0022351126908349062, 'gamma': 0.9024886193660144, 'switch_model_threshold': 10, 'optimizer': 'SGD', 'rho': 8.138388369927466}. Best is trial 11 with value: 0.04830917874396135.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:54:50,427]\u001b[0m Trial 14 finished with value: 0.027624309392265192 and parameters: {'batch_size': 15, 'epsilon': 0.18716615367363715, 'training_rounds': 149, 'num_hidden_layers': 161, 'learning_rate': 0.0020125232399838978, 'gamma': 0.9591783134219812, 'switch_model_threshold': 14, 'optimizer': 'SGD', 'rho': 9.826025341544751}. Best is trial 11 with value: 0.04830917874396135.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:56:18,970]\u001b[0m Trial 15 finished with value: 0.009478672985781991 and parameters: {'batch_size': 22, 'epsilon': 0.12855723421430432, 'training_rounds': 100, 'num_hidden_layers': 99, 'learning_rate': 0.0033335248221989322, 'gamma': 0.9893747629852913, 'switch_model_threshold': 16, 'optimizer': 'SGD', 'rho': 8.059698537201731}. Best is trial 11 with value: 0.04830917874396135.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:58:07,476]\u001b[0m Trial 16 finished with value: 0.0794392523364486 and parameters: {'batch_size': 16, 'epsilon': 0.16574045583954436, 'training_rounds': 146, 'num_hidden_layers': 59, 'learning_rate': 0.00012511517257887483, 'gamma': 0.9148364820029773, 'switch_model_threshold': 11, 'optimizer': 'Adam', 'rho': 9.617066946649322}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-07 23:59:00,729]\u001b[0m Trial 17 finished with value: 0.04964539007092199 and parameters: {'batch_size': 17, 'epsilon': 0.20363162538279703, 'training_rounds': 106, 'num_hidden_layers': 52, 'learning_rate': 0.0032969217831111263, 'gamma': 0.9105756874358463, 'switch_model_threshold': 13, 'optimizer': 'SGD', 'rho': 9.985901116494922}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:00:30,221]\u001b[0m Trial 18 finished with value: 0.010526315789473684 and parameters: {'batch_size': 17, 'epsilon': 0.20274657404384702, 'training_rounds': 138, 'num_hidden_layers': 48, 'learning_rate': 0.003529885338190912, 'gamma': 0.9146996554491543, 'switch_model_threshold': 13, 'optimizer': 'Adam', 'rho': 9.853322771255574}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:01:24,315]\u001b[0m Trial 19 finished with value: 0.027777777777777776 and parameters: {'batch_size': 14, 'epsilon': 0.20617108085682198, 'training_rounds': 114, 'num_hidden_layers': 52, 'learning_rate': 0.0065903446381359535, 'gamma': 0.9139230843422605, 'switch_model_threshold': 16, 'optimizer': 'SGD', 'rho': 9.238313791798628}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:02:29,712]\u001b[0m Trial 20 finished with value: 0.008064516129032258 and parameters: {'batch_size': 19, 'epsilon': 0.21769227175046307, 'training_rounds': 83, 'num_hidden_layers': 60, 'learning_rate': 0.0034577800470798037, 'gamma': 0.9132556309729541, 'switch_model_threshold': 13, 'optimizer': 'Adam', 'rho': 9.343605881900583}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:04:00,419]\u001b[0m Trial 21 finished with value: 0.014705882352941176 and parameters: {'batch_size': 22, 'epsilon': 0.17021770972427008, 'training_rounds': 104, 'num_hidden_layers': 96, 'learning_rate': 1.2603369052510341e-05, 'gamma': 0.9776468378531008, 'switch_model_threshold': 11, 'optimizer': 'SGD', 'rho': 9.93471311875787}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:04:28,951]\u001b[0m Trial 22 finished with value: 0.03773584905660377 and parameters: {'batch_size': 13, 'epsilon': 0.1895069253566933, 'training_rounds': 50, 'num_hidden_layers': 68, 'learning_rate': 0.0012383906786820086, 'gamma': 0.9004235953036202, 'switch_model_threshold': 10, 'optimizer': 'SGD', 'rho': 9.386594839042708}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:05:12,569]\u001b[0m Trial 23 finished with value: 0.021505376344086023 and parameters: {'batch_size': 18, 'epsilon': 0.15328949070215836, 'training_rounds': 114, 'num_hidden_layers': 33, 'learning_rate': 0.0027624673463511095, 'gamma': 0.9205100677826926, 'switch_model_threshold': 13, 'optimizer': 'SGD', 'rho': 8.557452649395547}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:06:45,312]\u001b[0m Trial 24 finished with value: 0.034482758620689655 and parameters: {'batch_size': 24, 'epsilon': 0.13061088429020573, 'training_rounds': 87, 'num_hidden_layers': 118, 'learning_rate': 0.0010351475256034398, 'gamma': 0.955286697115613, 'switch_model_threshold': 11, 'optimizer': 'SGD', 'rho': 9.994531037506242}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:08:40,794]\u001b[0m Trial 25 finished with value: 0.023880597014925373 and parameters: {'batch_size': 29, 'epsilon': 0.17246926911505256, 'training_rounds': 130, 'num_hidden_layers': 87, 'learning_rate': 0.004469661138710445, 'gamma': 0.9064034584983762, 'switch_model_threshold': 12, 'optimizer': 'SGD', 'rho': 8.785750646922848}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n","\u001b[32m[I 2020-12-08 00:09:11,896]\u001b[0m Trial 26 finished with value: 0.05172413793103448 and parameters: {'batch_size': 21, 'epsilon': 0.19243595182434672, 'training_rounds': 69, 'num_hidden_layers': 40, 'learning_rate': 0.006201641296040004, 'gamma': 0.9206368953729719, 'switch_model_threshold': 15, 'optimizer': 'SGD', 'rho': 9.62917700276578}. Best is trial 16 with value: 0.0794392523364486.\u001b[0m\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"IdFeO4C6oz0F"},"source":["# sns.lineplot(x='games', y='scores', data=score_results, label=f'Scores - batch_size: {batch_size} - training_rounds: {training_rounds} - num_hidden_layers: {num_hidden_layers}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rICiNnPWpgZ"},"source":["# g = sns.lineplot(x='rounds', y='loss', data=loss_results, label=f'Loss - batch_size: {batch_size} - training_rounds: {training_rounds} - num_hidden_layers: {num_hidden_layers}')\n","# g.set_yscale(\"log\")"],"execution_count":null,"outputs":[]}]}